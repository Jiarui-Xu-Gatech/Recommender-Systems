{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ExaOcnjFil"
      },
      "source": [
        "# CX 4803 / CSE 6240 Web Search and Text Mining\n",
        "## Homework 5: Recommender Systems\n",
        "\n",
        "This homework asks you to build various recommender systems based on real-world movie rating data called MovieLens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "7clA8ohorzLd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from scipy.spatial import distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7RbCrcMgk3J"
      },
      "source": [
        "Please run the following cell substituting your student and user names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "mHA5XXogr7b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f9bb53-c85b-4398-d148-9c0936c4ccfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I, Jiarui_Xu (jxu605), state that I performed the tasks in this assignment following the Georgia Tech honor code(https://osi.gatech.edu/content/honor-code).\n"
          ]
        }
      ],
      "source": [
        "def author_honor_code (student_name='Jiarui_Xu', user_name='jxu605'):\n",
        "  print (f'I, {student_name} ({user_name}), state that I performed the tasks in this assignment following the Georgia Tech honor code(https://osi.gatech.edu/content/honor-code).')\n",
        "\n",
        "# print the honor code before submission (substitute your name and username)\n",
        "author_honor_code ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNfxqaiiwG-i"
      },
      "source": [
        "## Section 1: **Data Parsing & Preprocessing** [1.0 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmQNWbeMBWLT"
      },
      "source": [
        "Lets Mount the notebook on the google drive. This lets you access and load files from your drive. When you run the below cell, you will be asked to connect this notebook to your google drive. Select your google account and then select _Allow_ to let this notebook access content of your google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "0dhTza5xBYE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b81d363-4017-4483-c570-d33441360cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV7LWdT7Bp0v"
      },
      "source": [
        "Set path to working directory. This is command line agrument. In python notebook, one can execute all the terminal commands by placing `!` or `%`in front of it. These are known as **magic commands**. Here we set the working directory to _HW5_ folder so that that we are able to access the datafiles with the help of relative path.\n",
        "To learn more about magic commands: [Read Documentation](https://ipython.readthedocs.io/en/stable/interactive/magics.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIGQArhXBtM2"
      },
      "source": [
        "**NOTE**: If you have different path for HW5 in your google drive, change the variable **`path`** accordingly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "k_DrH-0eBbtZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff63584-47e5-40ed-eda0-638bac2f8a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HW5\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/HW5'\n",
        "%cd $path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "aIA2YEv8BgRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542c6012-87d3-4077-b272-daedcb6cf5ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HW5\n"
          ]
        }
      ],
      "source": [
        "#Check if your present working directory has changed to the path specified in the previous cel.\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q4Q0D7kC4gK"
      },
      "source": [
        "### Data Description: **[MovieLens](https://files.grouplens.org/datasets/movielens/ml-latest-small-README.html)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kriRDSbej7XE"
      },
      "source": [
        "MovieLens dataset (ml-latest-small) contains 100836 ratings and 3683 tag applications across 9742 movies. In this homework, we offer two files `train.csv` and `test.csv`, which will be used for training and testing the model, respectively. Each line of these files represents one rating of one movie by one user, and has the following format:\n",
        "\n",
        "    userId,movieId,rating,timestamp\n",
        "\n",
        "The lines within this file are ordered first by userId, then, within user, by movieId.\n",
        "Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
        "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-RSlcYZwfSU"
      },
      "source": [
        "### 1.1 Write a function to parse and normalize the training and test data from files [0.5 points]\n",
        "Store only **user_id, movie_id, and ratings** since we will ignore timestamps in this homework.  \n",
        "**All ratings should be divided by 5**, so they are in 0-1 scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "X7ceuvqh2p7H"
      },
      "outputs": [],
      "source": [
        "def load_dataset (filename):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  filename (str): The name of the file which contains the movie ratings.\n",
        "\n",
        "  Returns:\n",
        "  processed_data (str-type numpy.array): the str-type numpy.array containing\n",
        "   - ex:) [user_id, movie_id, normalized rating (0-1)] in each row.\n",
        "\n",
        "  Steps:\n",
        "  1. split each row(type: str) into a list [userID,movieId,rating,timestamp] using delimiter ','.\n",
        "  2. append the list [userID,movieID,normalized rating] on input_data.\n",
        "  \"\"\"\n",
        "  input_data = []\n",
        "  with open (filename) as fin:\n",
        "    for i, line in enumerate (fin):\n",
        "      ## Add code below [0.5 points] ##\n",
        "      line2=line.split(',')\n",
        "      input_data.append([line2[0],line2[1],str(float(line2[2])/5)])\n",
        "      #################################\n",
        "  return np.array(input_data).astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "3YcuUUm1v4wZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501f1f93-53b4-47a6-c5e7-e7166adeefd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dataset statistics ===\n",
            "Number of Training Data: 90718\n",
            "Number of Test Data: 9715\n",
            "Min/max Ratings of Training Data: ('0.1', '1.0')\n",
            "======\n"
          ]
        }
      ],
      "source": [
        "training_data = load_dataset ('train.csv')\n",
        "test_data = load_dataset ('test.csv')\n",
        "print (f'=== Dataset statistics ===')\n",
        "print (f'Number of Training Data: {training_data.shape[0]}')\n",
        "print (f'Number of Test Data: {test_data.shape[0]}')\n",
        "print (f'Min/max Ratings of Training Data: {min(training_data[:,2]),max(training_data[:,2])}')\n",
        "print (f'======')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7Hj7JUiv8EG"
      },
      "source": [
        "**[Sanity Check]**  \n",
        "=== Dataset statistics ===  \n",
        "Number of Training Data: 90718  \n",
        "Number of Test Data: 9715  \n",
        "Min/max Ratings of Training Data: ('0.1', '1.0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F7mLVtou4bB"
      },
      "source": [
        "### 1.2 Write code to load pre-trained embeddings and rating dictionaries of each user and item  [0.5 points]\n",
        "Pre-trained user and item embeddings will be used in recommendation models \n",
        "to accelerate our computations.   \n",
        "Rating dictionaries of users and items will be used for item-item collaborative filtering and matrix factorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "Q44DW7QO5-x1"
      },
      "outputs": [],
      "source": [
        "def preprocessing(training_data):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  training data (str-type numpy.array): the training data containing user_id, movie_id, and normalized rating (0-1) information.\n",
        "\n",
        "  Returns:\n",
        "  R_ui (dictionary of dictionaries): this dictionary contains rating information of each user. \n",
        "   - The key is user_id (string), the value is a dictionary whose key is item_id (string) and value is rating (float).\n",
        "   - Thus, R_ui['1']['260'] = 1.0. R_ui should be computed with training data.\n",
        "  R_iu (dictionary of dictionaries): it is similar to R_ui, but the key of a dictionary is item_id (string). \n",
        "   - Thus, R_ui['260']['1'] = 1.0. R_iu should be computed with training data.\n",
        "  user_emb and item_emb (dictionaries of numpy.array): pre-trained embeddings of users and items. \n",
        "   - The keys are user_id (string) and item_id (string).\n",
        "   - The values are the corresponding embeddings (numpy.array; dim=32) of user_id and item_id, respectively.\n",
        "\n",
        "  Steps:\n",
        "  1. for each training example with (user u, item i, and rating r), R_ui[u][i] should be r (float). R_iu can be computed similarly.\n",
        "  2. Raw embedding files can be read by the pd.read_csv(file_name,header=None,sep=' ').values command.\n",
        "      - file name: 'pre_trained_user_emb', 'pre_trained_item_emb'\n",
        "  3. Convert the raw embeddings to dictionaries of numpy.array by taking the first column as keys and the rest of columns as values.\n",
        "      - ex:) for user_emb, [user_id, corresponding user embedding(dim=32)] > {user_id: corresponding user embedding(dim=32)}\n",
        "  \"\"\"\n",
        "  R_ui,R_iu = defaultdict(dict),defaultdict(dict)\n",
        "\n",
        "  ## Add code below [0.5 points] ##\n",
        "  user_emb={}\n",
        "  item_emb={}\n",
        "  for column in training_data:\n",
        "    R_ui[column[0]][column[1]]=float(column[2])\n",
        "    R_iu[column[1]][column[0]]=float(column[2])\n",
        "  pre_trained_user_emb=pd.read_csv('pre_trained_user_emb',header=None,sep=' ')\n",
        "  pre_trained_item_emb=pd.read_csv('pre_trained_item_emb',header=None,sep=' ')\n",
        "  for i in range(len(pre_trained_user_emb[0])):\n",
        "    user_emb[str(pre_trained_user_emb[0][i])]=np.zeros(32)\n",
        "    for j in range(32):\n",
        "      user_emb[str(pre_trained_user_emb[0][i])][j]=pre_trained_user_emb[j+1][i]\n",
        "  for i in range(len(pre_trained_item_emb[0])):\n",
        "    item_emb[str(pre_trained_item_emb[0][i])]=np.zeros(32)\n",
        "    for j in range(32):\n",
        "      item_emb[str(pre_trained_item_emb[0][i])][j]=pre_trained_item_emb[j+1][i]\n",
        "  #################################\n",
        "  return R_ui,R_iu,user_emb,item_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "S5_nVp1x1ZTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b700ebd-89f2-4445-c86a-bb57299d43cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R_ui of user 1 and item 260 = 1.0\n",
            "R_iu of user 1 and item 260 = 1.0\n",
            "Pre-trained embedding of user 1 = [ 0.516262  0.0786   -0.903148 -0.125587 -0.670954 -0.282254 -0.00769\n",
            " -0.455289  0.190866  0.143283 -0.10494  -0.489509 -0.740679  0.596688\n",
            "  0.491465  0.22938   0.757515 -0.131054 -0.046257  0.181893 -0.751261\n",
            "  0.448121 -0.315641 -0.09361  -0.229705  0.224912  0.065417  0.025272\n",
            " -0.070755 -0.075175 -0.090409 -0.365512]\n"
          ]
        }
      ],
      "source": [
        "R_ui,R_iu,user_emb,item_emb = preprocessing(training_data)\n",
        "print('R_ui of user 1 and item 260 = {}'.format(R_ui['1']['260']))\n",
        "print('R_iu of user 1 and item 260 = {}'.format(R_iu['260']['1']))\n",
        "print('Pre-trained embedding of user 1 = {}'.format(user_emb['1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEGR8yzx7O3J"
      },
      "source": [
        "**[Sanity check]**   \n",
        "R_ui of user 1 and item 260 = 1.0  \n",
        "R_iu of user 1 and item 260 = 1.0  \n",
        "pre-trained embedding of user 1 = [ 0.516262  0.0786   -0.903148 -0.125587 -0.670954 -0.282254 -0.00769\n",
        " -0.455289  0.190866  0.143283 -0.10494  -0.489509 -0.740679  0.596688\n",
        "  0.491465  0.22938   0.757515 -0.131054 -0.046257  0.181893 -0.751261\n",
        "  0.448121 -0.315641 -0.09361  -0.229705  0.224912  0.065417  0.025272\n",
        " -0.070755 -0.075175 -0.090409 -0.365512 ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usPfJliCPJdy"
      },
      "source": [
        "## Section 2 : **Item-Item Collaborative Filtering** [2.0 points]\n",
        "\n",
        "The first recommendation model we are developing is a collaborative filtering algorithm using an item-oriented approach. \n",
        "\n",
        "First, we use the cosine similarity to compute similarity between items $i$ and $j$ as follows.\n",
        "\n",
        "$$\n",
        "sim(i,j) = \\frac{E^{item}_{i} \\bullet E^{item}_{j}}{||E^{item}_{i}||_2 ||E^{item}_{j}||_2} \n",
        "$$\n",
        "\n",
        "where $E^{item}_{i}$ is the pre-trained embedding of item $i$, $\\bullet$ denotes an inner product between vectors, and $||X||_2$ is a L2-norm of a vector $X$.\n",
        "\n",
        "With the above similarity function, we can compute **the predicted rating** $P_{u,i}$ on an item $i$ for a user $u$ by computing the weighted sum of the ratings given by the user on the other items **except $i$**.\n",
        "$$P_{u,i} = \\frac{\\sum_{\\forall j \\neq i, (u,j) \\in R}(sim(i,j)R_{u,j})}{\\sum_{\\forall j \\neq i, (u,j) \\in R}(|sim(i,j)|)}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyteGLPm65Mo"
      },
      "source": [
        "### 2.1 Write a function to get the **similarity** of items $i$ and $j$ [0.5 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "n723xTjXhTNN"
      },
      "outputs": [],
      "source": [
        "def item_similarity(item_emb, item_i, item_j):\n",
        "    \"\"\"\n",
        "    Arguments: \n",
        "    item_emb (dictionary of numpy.array): pre-trained embeddings of items. \n",
        "     - The key is item_id (string), and the value is the corresponding item embedding (numpy.array; dim=32).\n",
        "    item_i, item_j (strings): two item_ids for similarity computation.\n",
        "\n",
        "    Returns:\n",
        "    sim(i,j) (float): cosine similarity of two embeddings of item i and j.\n",
        "\n",
        "    Steps: \n",
        "    1. You can use scipy.spatial.distance.cosine to compute the similarity.\n",
        "    \"\"\"\n",
        "    ## Add code below [0.5 points] ##\n",
        "    cosine_similarity=1-distance.cosine(item_emb[item_i],item_emb[item_j])\n",
        "    #################################\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgLwsoMglZ6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97f5351-3301-4a3a-8eac-1756319ffb18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sim(1,163) = 0.391101\n"
          ]
        }
      ],
      "source": [
        "print('sim(1,163) = {:.6f}'.format(item_similarity(item_emb,'1','163')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiJR31OtDvsJ"
      },
      "source": [
        "**[Sanity check]**     \n",
        "sim(1,163) = 0.391101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92qgZF0u8Fi7"
      },
      "source": [
        "### 2.2 Write a function to compute **the predicted rating $P_{u,i}$** of on an item $i$ for a user $u$ [1.0 points] \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "mqvoE7afkWYX"
      },
      "outputs": [],
      "source": [
        "def item_item_collaborative_filtering(item_emb, user_u, item_i):\n",
        "    \"\"\"\n",
        "    Arguments: \n",
        "    item_emb (dictionary of numpy.array): pre-trained embeddings of items. \n",
        "     - The key is item_id (string), and the value is the corresponding item embedding (numpy.array; dim=32).\n",
        "    user_u (string): user_id.\n",
        "    item_j (string): item_id.\n",
        "\n",
        "    Returns:\n",
        "    P_{u,i} (float): the predicted rating of user i on item j based on the item-item collaborative filtering.\n",
        "\n",
        "    Steps:\n",
        "    1. retrieve the set of items I a user rated using the keys of R_ui.\n",
        "    2. for each item in I (must be different from item_i), compute sim(current_item, item_i).\n",
        "    3. update numerator and denominator values for the current item based on the above P_{u,i} equation. \n",
        "        - Don't forget to use absolute value of the similarity while computing denominator.\n",
        "    4. repeat 2 and 3 for all items in I, and return numerator/denominator.\n",
        "    \"\"\"\n",
        "    numerator,denominator = 0,0\n",
        "    ## Add code below [1.0 points] ##\n",
        "    I=R_ui[user_u].keys()\n",
        "    for item in I:\n",
        "      if not item==item_i:\n",
        "        sim=item_similarity(item_emb, item, item_i)\n",
        "        numerator+=sim*float(R_ui[user_u][item])\n",
        "        denominator+=abs(sim)\n",
        "    #################################\n",
        "    return numerator/denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtGDfnQoriM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9219e53-20c8-445f-a719-e26601cc2087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P_(1,296) = 0.872167\n"
          ]
        }
      ],
      "source": [
        "print('P_(1,296) = {:.6f}'.format(item_item_collaborative_filtering(item_emb,'1','296')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA5o8uhOD9_j"
      },
      "source": [
        "**[Sanity check]**          \n",
        "P_(1,296) = 0.872167"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9l2s_KYFEf4"
      },
      "source": [
        "### 2.3 Write a function to measure **root-mean-square-error (RMSE)** on test data. [0.5 points] \n",
        "\n",
        "**Root-mean-square error (RMSE)** is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. \n",
        "Mathematically, test RMSE of a recommendation model for movie rating prediction is calculated as follows.\n",
        "$$ Test-RMSE = \\sqrt{\\frac{\\sum_{(u,i)\\in R^{test}} (R_{u,i}^{test} - P_{u,i})^2}{N}} $$\n",
        "where $R^{test}$ is test data and $N$ is the number of test data. Note that training RMSE is computed similarly to test RMSE on training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "yPS2P49kE_cg"
      },
      "outputs": [],
      "source": [
        "def test_RMSE_of_collaborative_filtering(test_data,item_emb):\n",
        "  \"\"\"\n",
        "  Arguments: \n",
        "  test data (str-type numpy.array): the test data containing user_id, movie_id, and normalized rating (0-1) information.\n",
        "  item_emb (dictionary of numpy.arraay): pre-trained embeddings of items. \n",
        "   - The key is item_id (string), and the value is the corresponding item embedding (numpy.array; dim=32).\n",
        "\n",
        "  Returns:\n",
        "  test_RMSE (float): the test RMSE of item-item collaborative filtering model.\n",
        "\n",
        "  Steps:\n",
        "  1. for each test example in the test data, compute P_{u,i} using the item_item_collaborative_filtering function.\n",
        "  2. compute the error (R_{u,i}^{test} - P_{u,i}) for the current test example.\n",
        "  3. sum the square of the error for all test examples.\n",
        "  4. divide the sum by the number of test examples and compute the root of it.\n",
        "  \"\"\"\n",
        "  test_RMSE = 0\n",
        "  ## Add code below [0.5 points] ##\n",
        "  square=0\n",
        "  for test_sample in test_data:\n",
        "    P_ui=item_item_collaborative_filtering(item_emb, test_sample[0], test_sample[1])\n",
        "    error=float(test_sample[2])-P_ui\n",
        "    square+=error*error\n",
        "  test_RMSE=np.sqrt(square/test_data.shape[0])\n",
        "  #################################\n",
        "  return test_RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLW7VFYHUANi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfcf11cf-d2fe-44e9-bdb2-2554bdf51588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_RMSE = 0.182710\n"
          ]
        }
      ],
      "source": [
        "print('test_RMSE = {:.6f}'.format(test_RMSE_of_collaborative_filtering(test_data,item_emb)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdiLidqPGGn_"
      },
      "source": [
        "**[Sanity check]**       \n",
        "   test RMSE = 0.182710"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_F77afKPiJq"
      },
      "source": [
        "## Section 3: **Matrix Factorization** [3.0 points]\n",
        "\n",
        "The second recommendation algorithm is called matrix factorization. Specifically, the matrix factorization algorithm works by decomposing the rating matrix into the product of two lower dimensionality rectangular matrices. Letâ€™s define $K$ as the number of latent factors (or reduced dimensionality). Then, we learn a user profile $U \\in  \\mathbb{R}^{N \\times K}$ and an item profile $V \\in  \\mathbb{R}^{M \\times K}$ ($N$ and $M$ are the number of users and items, respectively).\n",
        "We want to approximate a rating by an inner product of two length-$K$ vectors, one representing user profile and the other item profile. Mathematically, a\n",
        "rating $R_{u,i}$ of the user $u$ on the movie $i$ is approximated by\n",
        "$$R_{u,i} \\approx \\sum_{k=1}^{K} U_{u,k} V_{i,k}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSW3Za4Kl-2z"
      },
      "source": [
        "### 3.1 Write a function to learn **$U$ and $V$** with gradient descent. [2.0 points]\n",
        "\n",
        "We want to fit each element of $U$ and $V$ by minimizing squared reconstruction error over all training data points. That is, the objective function we minimize is given by\n",
        "$$ L(U,V,\\lambda) = \\sum_{(u,i) \\in R} (R_{u,i} - \\sum_{k=1}^{K} U_{u,k} V_{i,k})^2 + \\lambda \\sum_{u,k} U_{u,k}^2 + \\lambda \\sum_{i,k} V_{i,k}^2 $$\n",
        "where $U_{u}$ is the $u$th row of $U$ and $V_{i}$ is the $i$th row of $V$, and $\\lambda$ is a regularization hyperparameter controlling the degree of penalization of large values in $U$ and $V$. As $U$ and $V$ are interrelated, there is no closed form solution. Thus, we update each element of $U$ and $V$\n",
        "using the gradient descent formula.\n",
        "\n",
        "Since the gradient descent update is quite complicated, **we will provide the code of it**. \n",
        "We can iteratively update $U$ and $V$ by calling the gradient descent update function multiple times.\n",
        "Use the number of latent factors **$K=10$** and **initialize $U$ and $V$ randomly between 0 and $\\sqrt{\\frac{avg(R_{u,i})}{K}}$**, where $avg(R_{u,i})$ is the average ratings of all training examples in the training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "mAMKHA7RBoRl"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_update(U,V,K):\n",
        "  \"\"\"\n",
        "  Do not modify this function. There is a -2.0 point penalty if you modify this function.\n",
        "\n",
        "  Arguments: \n",
        "  U,V (dictionary of numpy.array): current user and item profile dictionaries. \n",
        "   - The key is either user_id or item_id, and the value is the corresponding user or item profile (numpy.array; dim:K).\n",
        "  K (int): the number of latent factors.\n",
        "\n",
        "  Returns:\n",
        "  Updated U,V (dictionary of numpy.array): updated user and item profile dictionaries. \n",
        "   - The key is either user_id or item_id, and the value is the corresponding user or item profile (numpy.array; dim:K).\n",
        "  \"\"\"\n",
        "  mu = 0.001\n",
        "  lambda_value = 0.001\n",
        "  for user in U.keys():\n",
        "    updates = np.zeros(K)\n",
        "    for item in R_ui[user].keys():\n",
        "      pred = np.inner(U[user],V[item])\n",
        "      error = R_ui[user][item] - pred\n",
        "      updates += error*V[item]\n",
        "    final_updates = 2*mu*updates - 2*lambda_value*U[user]\n",
        "    U[user] += final_updates\n",
        "\n",
        "  for item in V.keys():\n",
        "    updates = np.zeros(K)\n",
        "    for user in R_iu[item].keys():\n",
        "      pred = np.inner(U[user],V[item])\n",
        "      error = R_iu[item][user] - pred\n",
        "      updates += error*U[user]\n",
        "    final_updates = 2*mu*updates - 2*lambda_value*V[item]\n",
        "    V[item] += final_updates\n",
        "  return U,V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "OfU6_L7534a2"
      },
      "outputs": [],
      "source": [
        "def matrix_factorization (training_data, K=10, epochs = 200):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  training data (str-type numpy.array): the training data containing user_id, movie_id, and normalized rating (0-1) information.\n",
        "  K (int): number of latent factors used for matrix factorization.\n",
        "  epochs (int): number of repetitions of the updates of U and V.\n",
        "\n",
        "  Returns:\n",
        "  U,V (dictionary of float-type numpy.array): learned user and item profile dictionaries. \n",
        "   - The key is either user_id or item_id, and the value is the corresponding user or item profile (float-type numpy.array; dim:K).\n",
        "\n",
        "  Steps for the first code block:\n",
        "  1. compute the maximum value using 'sqrt(avg(ratings of all training examples)/K)' for the initialization (ratings of all training examples should be float-type, not str-type here).\n",
        "  2. for each user u in training_data, initialize the value of U[u] with a size-K numpy.array (float) filled with random values between 0 the maximum value.\n",
        "  3. initialize V[v] for each item v in training_data like step 2.\n",
        "      - when you assign the initial value, please use R_ui.keys(), R_iu.keys() to keep the order and to avoid multiple initialization.\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(0)\n",
        "  U,V = defaultdict(np.array),defaultdict(np.array)\n",
        "\n",
        "  ## Add code below [1.0 points] ##\n",
        "  sum_rating=0\n",
        "  for item in training_data:\n",
        "    sum_rating+=float(item[2])\n",
        "  maximum=np.sqrt((sum_rating/training_data.shape[0])/K)\n",
        "  for u in R_ui.keys():\n",
        "    U[u]=np.random.sample(K)*maximum\n",
        "  for v in R_iu.keys():\n",
        "    V[v]=np.random.sample(K)*maximum\n",
        "  #################################\n",
        "\n",
        "  \"\"\"\n",
        "  Steps for the second code block:\n",
        "  1. for each iteration, call the gradient_descent_update with current U and V.\n",
        "  2. update the user and item profile matrices with the returned U and V.\n",
        "  \"\"\"\n",
        "  ## Add code below [1.0 points] ##\n",
        "  for i in range(epochs):\n",
        "    U,V=gradient_descent_update(U,V,K)\n",
        "  #################################\n",
        "  return U,V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "cP_FCd96Dlwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5989735-3c80-45f4-8e0c-1b89a5aaaf4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the user profile of user 1 =\n",
            "[0.4514257  0.5078636  0.5814194  0.44791872 0.49933625 0.51698918\n",
            " 0.47100742 0.6167688  0.57151657 0.49267464]\n"
          ]
        }
      ],
      "source": [
        "(W,H) = matrix_factorization(training_data)\n",
        "print('the user profile of user 1 =\\n{}'.format(W['1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq6Op7UsQOrl"
      },
      "source": [
        "**[Sanity check]** (*The running time should be less than 10 minutes)   \n",
        "the user profile of the user 1 =      \n",
        "[0.4514257 , 0.5078636 , 0.5814194 , 0.44791872, 0.49933625, 0.51698918,      \n",
        "0.47100742, 0.6167688 , 0.57151657, 0.49267464]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKJZXUAPGIqR"
      },
      "source": [
        "### 3.2 Write a function to compute **training/test RMSE** of the matrix factorization model. [1.0 points]\n",
        "\n",
        "Compute RMSE values of the matrix factorization model on training/test data, and report the two RMSE values.  \n",
        "You can refer to Part 2.3 for the definition of training/test RMSE values.\n",
        "$$ RMSE = \\sqrt{\\frac{\\sum_{(u,i)\\in R} (R_{u,i} - P_{u,i})^2}{N}} $$\n",
        "where $R$, $P$ are ground-truth and predicted rating respectively, and $N$ is the number of examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "JqD_q_2IHQR3"
      },
      "outputs": [],
      "source": [
        "def training_and_test_RMSE_of_MF(training_data, test_data):\n",
        "  \"\"\"\n",
        "  Arguments: \n",
        "  training_data and test_data (str-type numpy.array): the training and test data containing user_id, movie_id, and normalized rating (0-1) information.\n",
        "\n",
        "  Returns:\n",
        "  training_RMSE and test_RMSE (float): the training and test RMSE of the matrix factorization model.\n",
        "\n",
        "  Steps for the first code block:\n",
        "  1. for each training example in the training data, compute P_{u,i} by the inner product of user and item profile vectors of u and i.\n",
        "  2. compute the error (R_{u,i} - P_{u,i}) for the current training example.\n",
        "  3. sum the square of the error for all training examples.\n",
        "  4. derive training RMSE by dividing the sum by the number of training examples and computing the root of it.\n",
        "  \"\"\"\n",
        "  training_RMSE,test_RMSE = 0,0\n",
        "\n",
        "  ## Add code below [0.5 points] ##\n",
        "  #U,V=matrix_factorization(training_data)\n",
        "  sum_error=0\n",
        "  for example in training_data:\n",
        "    P_ui=np.inner(W[example[0]],H[example[1]])\n",
        "    error=float(example[2])-P_ui\n",
        "    sum_error+=np.square(error)\n",
        "  training_RMSE=np.sqrt(sum_error/training_data.shape[0])\n",
        "  #################################\n",
        "\n",
        "  \"\"\"\n",
        "  Steps for the second code block:\n",
        "  1. for each test example in the test data, compute P_{u,i} by the inner product of user and item profile vectors of u and i.\n",
        "  2. compute the error (R_{u,i} - P_{u,i}) for the current test example.\n",
        "  3. sum the square of the error for all test examples.\n",
        "  4. derive test RMSE by dividing the sum by the number of test examples and computing the root of it.\n",
        "  \"\"\"\n",
        "  ## Add code below [0.5 points] ##\n",
        "  #U2,V2=matrix_factorization(test_data)\n",
        "  sum2_error=0\n",
        "  for example in test_data:\n",
        "    P_ui2=np.inner(W[example[0]],H[example[1]])\n",
        "    error2=float(example[2])-P_ui2\n",
        "    sum2_error+=np.square(error2)\n",
        "  test_RMSE=np.sqrt(sum2_error/test_data.shape[0])\n",
        "  #################################\n",
        "  return training_RMSE,test_RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "vUCemVn2kVIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "941687f0-5a59-4d60-bbe0-beff554281c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training RMSE = 0.164524\n",
            "test RMSE = 0.180798\n"
          ]
        }
      ],
      "source": [
        "training_RMSE, test_RMSE = training_and_test_RMSE_of_MF(training_data,test_data)\n",
        "print('training RMSE = {:.6f}'.format(training_RMSE))\n",
        "print('test RMSE = {:.6f}'.format(test_RMSE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki3XXmLGQ82d"
      },
      "source": [
        "**[Sanity check]**       \n",
        "training RMSE = 0.164524        \n",
        "test RMSE = 0.180798"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efcxjWNjPsXF"
      },
      "source": [
        "## Section 4: **Deep learning-based Recommendation** [4.0 points]\n",
        "\n",
        "Finally, we will utilize a neural network that can accurately predict ratings and recommend items to users. We will implement a simple multi-layer perceptron (MLP) model with PyTorch library, which offers simple and straightforward neural network training process.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1kEFTCa7ragS44aYmF89V958TbJpcPv-u'>\n",
        "\n",
        "As shown in the above figure, the MLP model consists of **embedding layer, hidden layer** (or neural CF layers in the figure), and **output layer**. \n",
        "\n",
        "The embedding layer transforms user_id and item_id to user and item latent vectors, respectively. Although the weights of the embedding layer are usually trainable, we will use **pre-trained embeddings** of users and items for faster learning speed. \n",
        "\n",
        "The hidden layer consists of **multiple fully connected (FC) layers** with different sizes. Each FC layer 1) takes previous layer's output as input, 2) multiply them with its weight, and 3) applies an activation function like ReLU, Sigmoid, etc. (also bias term), and 4) send the output to the next layer. In this homework, we will use **two FC layers with size of [64,512] and [512,512]** and **ReLU activation function**. \n",
        "\n",
        "The output layer is also **fully-connected (with ReLU activation) with size of [512,1]** to produce a single prediction value **since our task is a regression problem**. The error between the ground-truth value and our prediction will be used for **backpropagation**, which updates all **trainable parameters** (e.g., weight matrices of FC layers) of the MLP model. After enough iterations, we will have trained parameters and can use them for predicing ratings for new items. In our mini-batch training setting, we will compare ground-truth ratings in the training batch and our prediction values of the current batch. \n",
        "\n",
        "Fortunately, we do not need to calculate the error and gradients for backpropagation by ourselves. **PyTorch** offers very simple functions for defining and training the MLP model! Please refer to the following documentation https://pytorch.org/tutorials/beginner/basics/intro.html for detailed information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qG2nZR9C4Ji"
      },
      "source": [
        "### 4.1 Write a class to **define the MLP model**. [1.0 points] \n",
        "\n",
        "As mentioned above, the MLP class consists of **two hidden layers (fully-connected with ReLU activation)** and **one output layer** (which is also fully-connected with ReLU activation). \n",
        "We will define the each component of the MLP model with PyTorch commands (e.g., nn.Linear() and nn.ReLU()), and we will also write a prediction function for a given input data, which will be used for model updates and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "ddX4p7EwIrwf"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Arguments: \n",
        "        self: the MLP model class.\n",
        "\n",
        "        Steps:\n",
        "        1. define the first hidden layer (self.fc1) with size of [64,512] (use nn.Linear()).\n",
        "        2. define the second hidden layer (self.fc2) with size of [512,512] (use nn.Linear()).\n",
        "        3. define the output layer (self.output) with size of [512,1] (use nn.Linear()).\n",
        "        4. define the ReLU activation layer (use nn.ReLU() with default hyperparameters). \n",
        "            - Please use one ReLU layer for the MLP model, instead of using multiple ReLU layers.\n",
        "        \"\"\"\n",
        "        \n",
        "        torch.manual_seed(0)\n",
        "        np.random.seed(0)\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.output = None\n",
        "        self.relu = None\n",
        "\n",
        "        ## Add code below [0.5 points] ##\n",
        "        self.fc1=nn.Linear(64,512)\n",
        "        self.fc2=nn.Linear(512,512)\n",
        "        self.output=nn.Linear(512,1)\n",
        "        self.relu=nn.ReLU()\n",
        "        #################################\n",
        "\n",
        "    def forward(self, input_emb):\n",
        "        \"\"\"\n",
        "        Arguments: \n",
        "        self: the MLP model class.\n",
        "        input_emb: the input data which is a concatenation of user and item embeddings.\n",
        "      \n",
        "        Returns:\n",
        "        prediction (torch.FloatTensor): the prediction values (torch.FloatTensor format) for a given input.\n",
        "\n",
        "        Steps:\n",
        "        1. compute the first intermediate output by feeding the input_emb to the first hidden layer (self.fc1) and applying ReLU activation.\n",
        "        2. compute the second intermediate output by feeding the first intermediate output to the second hidden layer (self.fc2) and applying ReLU activation. \n",
        "        3. compute the prediction values by feeding the second intermediate output to the output layer (self.output) and applying ReLU activation.\n",
        "        4. return the prediction values.\n",
        "        \"\"\"\n",
        "        ## Add code below [0.5 points] ##\n",
        "        output1=self.fc1(input_emb)\n",
        "        output1=self.relu(output1)\n",
        "        output2=self.fc2(output1)\n",
        "        output2=self.relu(output2)\n",
        "        prediction=self.output(output2)\n",
        "        prediction=self.relu(prediction)\n",
        "        output=prediction\n",
        "        #################################\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "re7x4sblwCZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b370e8-643a-4a83-e642-bd7316d17dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (output): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "MLP_model = MLP()\n",
        "print(MLP_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNEHfAQZaG68"
      },
      "source": [
        "**[Sanity check]** (*The output should be like the below (the layer names can be different)\n",
        "\n",
        "MLP(  \n",
        "  (fc1): Linear(in_features=64, out_features=512, bias=True)  \n",
        "  (fc2): Linear(in_features=512, out_features=512, bias=True)  \n",
        "  (output): Linear(in_features=512, out_features=1, bias=True)  \n",
        "  (relu): ReLU()  \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmTCF6GOHDp-"
      },
      "source": [
        "#### 4.2 Write a function to **initialize and train the MLP model**. [2.0 points] \n",
        "\n",
        "The detailed steps for initializing and training the MLP model are given as follows.  \n",
        "\n",
        "Step 1) Define the MLP model using the above class.  \n",
        "Step 2) Construct input data for the MLP model by concatenating the user and item embeddings.  \n",
        "Step 3) Feed the input data to the MLP and obtain the prediction values.  \n",
        "Step 4) Perform Backpropagation to update the model parameters.   \n",
        "Step 5) Repeat Steps 3-4 for several epochs (or iterations) and print training RMSE every 10 epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "6g1eFqRuHcvy"
      },
      "outputs": [],
      "source": [
        "def MLP_train(training_data,user_emb,item_emb,MLP_model,epochs=200):\n",
        "  \"\"\"\n",
        "  Arguments: \n",
        "  training_data (str-type numpy.array): the training data containing user_id, movie_id, and normalized rating (0-1) information.\n",
        "  user_emb and item_emb (dictionaries of numpy.array): pre-trained embeddings of users and items. \n",
        "   - The keys are user_id (string) and item_id (string), and the values are the corresponding embeddings (numpy.array; dim=32) of user_id and item_id, respectively.\n",
        "  MLP_model (MLP class): the untrained MLP model.\n",
        "  epochs (int): number of iterations required for updating the MLP model.\n",
        "  \n",
        "  Returns:\n",
        "  MLP_model (MLP class): the MLP model trained with given training data and pre-trained embeddings.\n",
        "\n",
        "  Steps for the first code block:\n",
        "  1. for each training example, retrieve pre-trained embeddings for a user u and item i.\n",
        "  2. concatenate those embeddings by np.concatenate((user_emb_of_u,item_emb_of_i)) and append it to a list.\n",
        "  3. convert a list of concatenated embeddings to a PyTorch FloatTensor by torch.FloatTensor(np.array(list)).\n",
        "  \"\"\"\n",
        "  torch.manual_seed(0)\n",
        "  np.random.seed(0)\n",
        "\n",
        "  input_emb = []\n",
        "  ## Add code below [0.5 points] ##\n",
        "  for example in training_data:\n",
        "    u=example[0]\n",
        "    v=example[1]\n",
        "    input_emb.append(np.concatenate((user_emb[u],item_emb[v])))\n",
        "  emb_input=torch.FloatTensor(np.array(input_emb))\n",
        "  #################################\n",
        "  \n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(MLP_model.parameters())\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    \"\"\"  \n",
        "    Steps for the second code block:\n",
        "    1. set the gradients to zero before backpropragation by optimizer.zero_grad().\n",
        "    2. call the prediction function of the MLP model by MLP_model(input_data).flatten() and obtain the prediction result for the input data, \n",
        "       where the input data is a PyTorch FloatTensor from Step 3 of the previous code block.\n",
        "    3. obtain the ground-truth rating values from training_data (in torch.FloatTensor format; use torch.FloatTensor(np.array) for type conversion).\n",
        "    4. compute the loss by loss=criterion(prediction,ground_truth).\n",
        "    5. perform backpropragation by loss.backward().\n",
        "    6. update the model parameters by optimizer.step().\n",
        "    7. compute the differences between the prediction and ground_truth by (prediction-ground_truth).detach().numpy().\n",
        "    8. compute and print training RMSE using the differences for every 10 epochs (epoch 0, epoch 10, ..., epoch 190).\n",
        "        - Don't use loss in Step 4 to derive training RMSE\n",
        "    \"\"\"\n",
        "    ## Add code below [1.5 points] ##\n",
        "    optimizer.zero_grad()\n",
        "    prediction=MLP_model(emb_input).flatten()\n",
        "    ground_truth=torch.FloatTensor(training_data[:,2].astype(np.float64))\n",
        "    loss=criterion(prediction,ground_truth)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    difference=(prediction-ground_truth).detach().numpy()\n",
        "    if epoch%10==0:\n",
        "      sum = 0\n",
        "      num = 0\n",
        "      for error in difference:\n",
        "        sum += np.square(error)\n",
        "        num += 1\n",
        "      train_RMSE = np.sqrt(sum/num)\n",
        "      print('@ epoch :',epoch)\n",
        "      print('train_RMSE:',train_RMSE)\n",
        "    #################################\n",
        "  return MLP_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "5TgNvqtTzviP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e67ebf-02b1-47c5-bda5-ff72a719de78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@ epoch : 0\n",
            "train_RMSE: 0.7276686443456213\n",
            "@ epoch : 10\n",
            "train_RMSE: 0.26427001787751536\n",
            "@ epoch : 20\n",
            "train_RMSE: 0.20403357031448344\n",
            "@ epoch : 30\n",
            "train_RMSE: 0.19665153696338888\n",
            "@ epoch : 40\n",
            "train_RMSE: 0.1950174310652665\n",
            "@ epoch : 50\n",
            "train_RMSE: 0.18971142885549117\n",
            "@ epoch : 60\n",
            "train_RMSE: 0.18791412144294312\n",
            "@ epoch : 70\n",
            "train_RMSE: 0.18545279366003994\n",
            "@ epoch : 80\n",
            "train_RMSE: 0.18328055667826693\n",
            "@ epoch : 90\n",
            "train_RMSE: 0.18133406298295532\n",
            "@ epoch : 100\n",
            "train_RMSE: 0.17948955019813978\n",
            "@ epoch : 110\n",
            "train_RMSE: 0.17779194069933832\n",
            "@ epoch : 120\n",
            "train_RMSE: 0.1762556726172096\n",
            "@ epoch : 130\n",
            "train_RMSE: 0.17487426905641276\n",
            "@ epoch : 140\n",
            "train_RMSE: 0.17361098129822228\n",
            "@ epoch : 150\n",
            "train_RMSE: 0.1724533243367808\n",
            "@ epoch : 160\n",
            "train_RMSE: 0.17137432608297912\n",
            "@ epoch : 170\n",
            "train_RMSE: 0.17034822884743708\n",
            "@ epoch : 180\n",
            "train_RMSE: 0.16936454061878048\n",
            "@ epoch : 190\n",
            "train_RMSE: 0.16841782512579018\n"
          ]
        }
      ],
      "source": [
        "trained_MLP_model = MLP_train(training_data,user_emb,item_emb,MLP_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYthR-6WaITN"
      },
      "source": [
        "**[Sanity check]** (*The running time should be less than 30 minutes)       \n",
        "train_RMSE @ epoch 0 should be around 0.727669.  \n",
        "train_RMSE @ epoch 10 should be around 0.264270.  \n",
        "train_RMSE @ epoch 20 should be around 0.204034.  \n",
        "train_RMSE @ epoch 190 should be around 0.168418.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeQW0oehH_2e"
      },
      "source": [
        "#### 4.3 Write a function to compute **test RMSE** of the MLP model. [1.0 points]\n",
        "\n",
        "Compute and report the test RMSE of the MLP model on test data.  \n",
        "You can refer to Part 2.3 for the definition of training/test RMSE values.\n",
        "$$ RMSE = \\sqrt{\\frac{\\sum_{(u,i)\\in R} (R_{u,i} - P_{u,i})^2}{N}} $$\n",
        "where $R$, $P$ are ground-truth and predicted rating respectively, and $N$ is the number of examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "TdBI0vKcIH9h"
      },
      "outputs": [],
      "source": [
        "def test_RMSE_of_MLP(test_data,user_emb,item_emb,MLP_model):\n",
        "  \"\"\"\n",
        "  Arguments: \n",
        "  test_data (str-type numpy.array): the test data containing user_id, movie_id, and normalized rating (0-1) information.\n",
        "  user_emb and item_emb (dictionaries of numpy.array): pre-trained embeddings of users and items. \n",
        "   - The keys are user_id (string) and item_id (string), and the values are the corresponding embeddings (numpy.array; dim=32) of user_id and item_id, respectively.\n",
        "  MLP_model (the MLP class): the trained MLP model.\n",
        "\n",
        "  Returns:\n",
        "  test_RMSE (float): the test RMSE of the MLP model.\n",
        "\n",
        "  Steps for the first code block:\n",
        "  1. for each test example, retrieve pre-trained embeddings for a user u and item i.\n",
        "  2. concatenate those embeddings by np.concatenate((user_emb_of_u,item_emb_of_i)) and append it to a list.\n",
        "  3. convert a list of concatenated embeddings to a PyTorch FloatTensor by torch.FloatTensor(np.array(list))\n",
        "  \"\"\"\n",
        "  \n",
        "  ## Add code below [0.5 points] ##\n",
        "  input_emb = []\n",
        "  for example in test_data:\n",
        "    u=example[0]\n",
        "    v=example[1]\n",
        "    input_emb.append(np.concatenate((user_emb[u],item_emb[v])))\n",
        "  emb_input=torch.FloatTensor(np.array(input_emb))\n",
        "  #################################\n",
        "\n",
        "  \"\"\"\n",
        "  Steps for the second code block:\n",
        "  1. call the prediction function of the trained MLP model by MLP(input_data).flatten() and obtain the prediction result for the input data, \n",
        "     where the input data is a PyTorch FloatTensor from Step 3 of the previous code block.\n",
        "  2. obtain the ground truth rating values from test_data.\n",
        "  3. compute the differences between the prediction and ground_truth by (prediction-ground_truth).detach().numpy().\n",
        "  4. compute the test RMSE using the differences and return the value.\n",
        "  \"\"\"\n",
        "  ## Add code below [0.5 points] ##\n",
        "  prediction=MLP_model(emb_input).flatten()\n",
        "  ground_truth=torch.FloatTensor(test_data[:,2].astype(np.float64))\n",
        "  difference=(prediction-ground_truth).detach().numpy()\n",
        "  sum = 0\n",
        "  num = 0\n",
        "  for error in difference:\n",
        "    sum += np.square(error)\n",
        "    num += 1\n",
        "  test_RMSE = np.sqrt(sum/num)\n",
        "  #################################\n",
        "  return test_RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "cy6ChLk5TbaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e640a8c6-77fa-4cfa-aa56-52a23fb465f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test RMSE = 0.172493\n"
          ]
        }
      ],
      "source": [
        "print('test RMSE = {:.6f}'.format(test_RMSE_of_MLP(test_data,user_emb,item_emb,trained_MLP_model)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2-6W0iJaJQz"
      },
      "source": [
        "**[Sanity check]**       \n",
        "test RMSE = 0.172493"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "assignment5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}